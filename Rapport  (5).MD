# EL Amraoui Abir ENCG SETTAT Finance G2                    
# Apog√©: 24010353
![Abir](https://github.com/elamraouiabir6963encg-rgb/24010353_ELAMRAOUI-ABIR/blob/main/Image/Abir.png?raw=true)

# üè¶ Analyse Exploratoire et Mod√©lisation Pr√©dictive  
## Dataset Bank Marketing (UCI)

---

## 1. Le Contexte M√©tier et la Mission

### Le Probl√®me (Business Case)
Les banques utilisent des campagnes de t√©l√©‚Äëmarketing pour convaincre les clients de souscrire √† un d√©p√¥t √† terme. Or :
    *   Ces campagnes co√ªtent cher,
    *   La majorit√© des clients ne r√©pondent pas favorablement,
    *   Un mauvais ciblage = perte d‚Äôargent + surcharge des centres d‚Äôappels. 
*   **Objectif :** Cr√©er un "Assistant IA" pour pr√©dire quels clients sont susceptibles de souscrire afin d'optimiser les campagnes.
*   **L'Enjeu critique :** La matrice des co√ªts d'erreur est asym√©trique.
    *   Faux Positif: On appelle un client qui ne sera jamais int√©ress√© ‚Üí perte de temps et co√ªt op√©rationnel.
    *   Faux N√©gatif: on rate un client r√©ellement int√©ress√© ‚Üí manque √† gagner commercial. **L'IA doit donc maximiser le rappel pour ne pas manquer des clients int√©ress√©s.**

### Les Donn√©es (L'Input)
On utilise le *Bank Marketing (UCI)*.
*   Profil client : √¢ge, m√©tier, √©ducation‚Ä¶
*   Situation financi√®re : solde bancaire
*   Historique marketing : nombre de contacts, r√©sultat des anciennes campagnes‚Ä¶
*   cible : y = oui/non (a souscrit ou non)
*   Format : 45 211 observations, 17 colonnes.
---
## 2. Le Code Python simplifi√©

```python
# INSTALLATION & IMPORTS
# pip install ucimlrepo seaborn matplotlib pandas numpy

from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
# 1) CHARGEMENT DES DONN√âES
# ============================================================
bank_marketing = fetch_ucirepo(id=222)
X = bank_marketing.data.features
y = bank_marketing.data.targets

df = pd.concat([X, y], axis=1)


# ============================================================
# 2) ANALYSE DES VARIABLES NUMERIQUES
# ============================================================
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# HISTOGRAMMES
for col in num_cols:
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution de {col}")
    plt.show()

# BOXPLOTS
for col in num_cols:
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot de {col}")
    plt.show()

# HEATMAP
plt.figure(figsize=(10,6))
corr = df[num_cols].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title("Matrice de corr√©lation")
plt.show()

# ============================================================
# 3) FEATURE ENGINEERING
# ============================================================
df['age_group'] = pd.cut(df['age'], bins=[0,30,45,60,100],
                          labels=['Jeune', 'Adulte', 'Senior', 'Tr√®s senior'])
df['duration_min'] = df['duration'] / 60
df['total_contacts'] = df['campaign'] + df['previous']


# ============================================================
# 4) MODELISATION : LOGISTIC REGRESSION + RANDOM FOREST
# ============================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

df['y'] = df['y'].map({'yes':1, 'no':0})

X = df.drop(columns=['y'])
y = df['y']

num_cols = X.select_dtypes(include=['int64','float64']).columns
cat_cols = X.select_dtypes(include=['object','category']).columns

preprocess = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ],
    remainder='passthrough'
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# ============================================================
# MODELE 1 : R√âGRESSION LOGISTIQUE
# ============================================================
log_reg_model = Pipeline(steps=[
    ('prep', preprocess),
    ('model', LogisticRegression(max_iter=1000))
])

log_reg_model.fit(X_train, y_train)
y_pred_log = log_reg_model.predict(X_test)



# ============================================================
# MODELE 2 : RANDOM FOREST
# ============================================================
rf_model = Pipeline(steps=[
    ('prep', preprocess),
    ('model', RandomForestClassifier(n_estimators=300, random_state=42))
])

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

```
---

# 5. R√âSULTATS DE L'ANALYSE EXPLORATOIRE (EDA)

---

## 5.1 Aper√ßu des donn√©es
![Aper√ßu des donn√©es](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Aper%C3%A7u%20des%20donn%C3%A9es.png?raw=true)

## 5.2 Statistiques descriptives
![Statistiques descreptives](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Statistiques%20descreptives.png?raw=true)
üìå
Forte pr√©sence de valeurs extr√™mes ‚Üí besoin de prudence en mod√©lisation.

---

# 5.3 Histogrammes ‚Äì Distributions

### Age
![Distrtibution de Age](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20Age.png?raw=true)

üìå Interpr√©tation - age :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.


### Balance
![Distribbution de Balance](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20Balance.png?raw=true)
üìå Interpr√©tation - balance :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

### Day of week
![Distribbution de pdays_of_week](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20day_of_week.png?raw=true)

üìå Interpr√©tation - day_of_week :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

### Duration
![Distribbution de duration](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20duration.png?raw=true)

üìå Interpr√©tation - duration :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

### Campaign
![Distribbution de campaign](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20campaign.png?raw=true)

üìå Interpr√©tation - campaign :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

### Pdays
![Distribbution de pdays](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20pdays.png?raw=true)

üìå Interpr√©tation - pdays :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

### Previous
![Distribbution de previous](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20previous.png?raw=true)

üìå Interpr√©tation - previous :
- La distribution permet d‚Äôobserver la forme g√©n√©rale (sym√©trique, asym√©trique, extr√™mes).
- Une asym√©trie indique une concentration des valeurs vers une borne.
- La pr√©sence de pics sugg√®re des groupes de clients distincts.

---

# 5.4 BOXPLOTS 

### Age
![Boxplot de age](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20age.png?raw=true)

üìå Interpr√©tation - age :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Balance
![Boxplot de Balance](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20Balance.png?raw=true)
üìå Interpr√©tation - balance :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Day of week
![Boxplot de day_of_week](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20day%20of%20week.png?raw=true)

üìå Interpr√©tation - day_of_week :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Duration
![Boxplot de duration](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20duration.png?raw=true)

üìå Interpr√©tation - duration :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Campaign
![Boxplot de campaign](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20campaign.png?raw=true)

üìå Interpr√©tation - campaign :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Pdays
![Boxplot de pdays](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20pdays.png?raw=true)

üìå Interpr√©tation - pdays :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

### Previous
![Boxplot de previous](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Boxplot%20de%20previous.png?raw=true)

üìå Interpr√©tation - previous :
- Les points isol√©s repr√©sentent des valeurs aberrantes (outliers).
- Une bo√Æte large indique une forte dispersion.
- Une asym√©trie de la bo√Æte sugg√®re une distribution biais√©e.

# 5.5 HEATMAP ‚Äì Corr√©lations
![Matrice de corr√©lation](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Matrice%20de%20corr%C3%A9lation.png?raw=true)

üìå Interpr√©tation globale des corr√©lations :
- Une corr√©lation proche de 1 ou -1 indique une relation forte.
- Une valeur proche de 0 indique une ind√©pendance.
- Les relations fortes peuvent indiquer une redondance de variables.
- Utile pour la s√©lection des variables dans les mod√®les pr√©dictifs.

--- FEATURE ENGINEERING ---
‚úÖ Nouvelles variables cr√©√©es :
  age_group  duration_min  total_contacts
0    Senior      4.350000               1
1    Adulte      2.516667               1
2    Adulte      1.266667               1
3    Senior      1.533333               1
4    Adulte      3.300000               1

üìå Interpr√©tation des nouvelles variables :
- age_group : segmentation client plus claire.
- duration_min : plus interpr√©table que les secondes.
- total_contacts : estimation de la pression commerciale sur le client.

### R√©partition par √¢ge
![R√©partition par groupe d'√¢ge](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/R%C3%A9partition%20par%20groupe%20d'%C3%A2ge.png?raw=true)

üìå Interpr√©tation : Les adultes dominent le portefeuille clients.

### Dur√©e (minutes)
![Distribution de la dur√©e de contact](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20de%20la%20dur%C3%A9e%20de%20contact.png?raw=true)

üìå Interpr√©tation : Les appels courts dominent ‚Äî peu d‚Äôappels longs tr√®s influents.

### Total contacts
![Distribution du nobre total de contacts](https://github.com/elamraouiabir6963encg-rgb/Projet-Dataset-Machine-learning/blob/main/Image/Distribution%20du%20nombre%20total%20de%20contacts.png?raw=true)

üìå Interpr√©tation : La majorit√© des clients est contact√©e peu de fois, signe de ciblage s√©lectif.

### R√âGRESSION LOGISTIQUE
![R√©ression logisitique](https://github.com/elamraouiabir6963encg-rgb/24010353_ELAMRAOUI-ABIR/blob/main/Image/R%C3%A9gression%20logistique.png?raw=true)

üìå Interpr√©tation :  
* La r√©gression logistique fournit une base simple pour pr√©dire l'abonnement.
* Si le recall sur la classe 1 (yes) est faible, le mod√®le a du mal √† d√©tecter les clients positifs.
* Une bonne pr√©cision signifie peu de faux positifs.
* Ce mod√®le est lin√©aire : il peut sous-performer si les relations ne sont pas lin√©aires.

### RANDOM FOREST
![Random forest](https://github.com/elamraouiabir6963encg-rgb/24010353_ELAMRAOUI-ABIR/blob/main/Image/Random%20forest.png?raw=true)

üìå Interpr√©tation :
* Ce mod√®le est souvent plus performant car il capture les non-lin√©arit√©s.
* Il r√©duit le surapprentissage gr√¢ce √† l‚Äôagr√©gation de nombreux arbres.
* Si le recall et la pr√©cision sont sup√©rieurs √† ceux de la r√©gression logistique, le mod√®le est meilleur.
* Random Forest est g√©n√©ralement le meilleur choix sur Bank Marketing.

---

# 6. INTERPR√âTATION G√âN√âRALE

| Point               | Conclusion             |
| ------------------- | ---------------------- |
| Dataset             | fiable                 |
| Outliers            | nombreux               |
| Corr√©lation         | faible sauf `duration` |
| Feature engineering | pertinent              |
| Probl√®me potentiel  | fuite de donn√©es       |

---

# 7. CONCLUSION
* L‚ÄôEDA r√©v√®le la distribution, la pr√©sence potentielle d‚Äôoutliers et les relations entre variables.
* Le feature engineering enrichit les donn√©es pour am√©liorer les mod√®les.
* La r√©gression logistique sert de mod√®le de base lin√©aire.
* Le Random Forest capture des relations plus complexes et donne souvent de meilleurs r√©sultats.
* La comparaison des m√©triques permet de choisir le meilleur mod√®le pour la pr√©diction de l'abonnement.
---

## Pistes d‚Äôam√©lioration

* Imputation intelligente
* Encodage cat√©goriel
* Standardisation
* S√©lection de variables
* Validation crois√©e
* Mod√©lisation ML (Logistic, Random Forest, XGBoost)

---

‚úÖ **Analyse exploratoire finalis√©e avec succ√®s.**

