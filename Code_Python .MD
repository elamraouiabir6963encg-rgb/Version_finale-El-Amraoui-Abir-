## Code utilis√©

```python
# INSTALLATION & IMPORTS
# pip install ucimlrepo seaborn matplotlib pandas numpy

from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
# 1) CHARGEMENT DES DONN√âES
# ============================================================
bank_marketing = fetch_ucirepo(id=222)
X = bank_marketing.data.features
y = bank_marketing.data.targets

df = pd.concat([X, y], axis=1)

print("\n=== Aper√ßu des donn√©es ===")
print(df.head())
print("\n=== Informations g√©n√©rales ===")
print(df.info())
print("\n=== Statistiques descriptives ===")
print(df.describe())

# ============================================================
# 2) ANALYSE DES VARIABLES NUMERIQUES
# ============================================================
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# HISTOGRAMMES
for col in num_cols:
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution de {col}")
    plt.show()
    print(f"\nüìå Commentaire Histogramme ({col}):")
    print(f" - La variable '{col}' pr√©sente sa distribution ci-dessus.")
    print(" - L‚Äôhistogramme permet d‚Äôobserver sa forme (normale, asym√©trique, aplatie...).")
    print(" - La courbe KDE aide √† visualiser les densit√©s et √©ventuels pics.\n")

# BOXPLOTS
for col in num_cols:
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot de {col}")
    plt.show()
    print(f"\nüìå Commentaire Boxplot ({col}):")
    print(f" - Le boxplot indique la distribution et les valeurs extr√™mes de '{col}'.")
    print(" - Les points situ√©s √† l‚Äôext√©rieur indiquent des potentiels outliers.\n")

# HEATMAP
plt.figure(figsize=(10,6))
corr = df[num_cols].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title("Matrice de corr√©lation")
plt.show()

print("\nüìå Commentaire Heatmap :")
print(" - La matrice montre les corr√©lations entre toutes les variables num√©riques.")
print(" - Les corr√©lations fortes (positives ou n√©gatives) peuvent indiquer des relations utiles pour la mod√©lisation.\n")

# ============================================================
# 3) FEATURE ENGINEERING
# ============================================================
df['age_group'] = pd.cut(df['age'], bins=[0,30,45,60,100],
                          labels=['Jeune', 'Adulte', 'Senior', 'Tr√®s senior'])
df['duration_min'] = df['duration'] / 60
df['total_contacts'] = df['campaign'] + df['previous']

print("\nüìå Feature Engineering effectu√© :")
print(" - Cr√©ation de 'age_group' pour regrouper les √¢ges.")
print(" - Conversion de 'duration' en minutes.")
print(" - Cr√©ation de 'total_contacts' : somme de campaign + previous.\n")

# ============================================================
# 4) MODELISATION : LOGISTIC REGRESSION + RANDOM FOREST
# ============================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

df['y'] = df['y'].map({'yes':1, 'no':0})

X = df.drop(columns=['y'])
y = df['y']

num_cols = X.select_dtypes(include=['int64','float64']).columns
cat_cols = X.select_dtypes(include=['object','category']).columns

preprocess = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ],
    remainder='passthrough'
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# ============================================================
# MODELE 1 : R√âGRESSION LOGISTIQUE
# ============================================================
log_reg_model = Pipeline(steps=[
    ('prep', preprocess),
    ('model', LogisticRegression(max_iter=1000))
])

log_reg_model.fit(X_train, y_train)
y_pred_log = log_reg_model.predict(X_test)

print("\n======================")
print("üìå R√âSULTATS : R√âGRESSION LOGISTIQUE")
print("======================")
print(classification_report(y_test, y_pred_log))
print("Matrice de confusion :")
print(confusion_matrix(y_test, y_pred_log))

print("\nüìå Commentaire sur la R√©gression Logistique :")
print(" - La r√©gression logistique fournit une base simple pour pr√©dire l'abonnement.")
print(" - Si le recall sur la classe 1 (yes) est faible, le mod√®le a du mal √† d√©tecter les clients positifs.")
print(" - Une bonne pr√©cision signifie peu de faux positifs.")
print(" - Ce mod√®le est lin√©aire : il peut sous-performer si les relations ne sont pas lin√©aires.\n")

# ============================================================
# MODELE 2 : RANDOM FOREST
# ============================================================
rf_model = Pipeline(steps=[
    ('prep', preprocess),
    ('model', RandomForestClassifier(n_estimators=300, random_state=42))
])

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

print("\n======================")
print("üå≤ R√âSULTATS : RANDOM FOREST")
print("======================")
print(classification_report(y_test, y_pred_rf))
print("Matrice de confusion :")
print(confusion_matrix(y_test, y_pred_rf))

print("\nüìå Commentaire sur le Random Forest :")
print(" - Ce mod√®le est souvent plus performant car il capture les non-lin√©arit√©s.")
print(" - Il r√©duit le surapprentissage gr√¢ce √† l‚Äôagr√©gation de nombreux arbres.")
print(" - Si le recall et la pr√©cision sont sup√©rieurs √† ceux de la r√©gression logistique, le mod√®le est meilleur.")
print(" - Random Forest est g√©n√©ralement le meilleur choix sur Bank Marketing.\n")

# ============================================================
# üìå COMMENTAIRE GLOBAL FINAL
# ============================================================
print("\nüéØ CONCLUSION GLOBALE :")
print(" - L‚ÄôEDA r√©v√®le la distribution, la pr√©sence potentielle d‚Äôoutliers et les relations entre variables.")
print(" - Le feature engineering enrichit les donn√©es pour am√©liorer les mod√®les.")
print(" - La r√©gression logistique sert de mod√®le de base lin√©aire.")
print(" - Le Random Forest capture des relations plus complexes et donne souvent de meilleurs r√©sultats.")
print(" - La comparaison des m√©triques permet de choisir le meilleur mod√®le pour la pr√©diction de l'abonnement.\n")
